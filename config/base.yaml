defaults:
  - prompter: tiny_llama_rag  # one of: tiny_llama_rag llama2, tiny_llama (see corresponding .yaml in conf/prompter/)
  - target_llm: llama-3.2-3b-instruct  # one of:  Qwen2.5-Coder-7B-Instruct CodeLlama-7b-Python-hf llama-3.2-3b-instruct llama2_chat, vicuna_chat, mistral_chat, pythia_chat, falcon_chat, gemma_chat, tiny_llama_chat (see corresponding .yaml in conf/target_llm/)
  - local_llm: llama-3.2-3b-instruct
  - _self_


gpu_list: [0,1,2,3,4,5,6,7]
seed: 12




verbose: false
reweight_loss: true
output_dir: "./exp/local/${now:%Y.%m.%d}/${now:%H%M}"

data:
  data_dir: "./data"
  test_prefixes_pth: "data/jailbreak_temp/test_prefixes.csv"
  affirmative_prefixes_pth: "data/jailbreak_temp/affirmative_prefixes.csv"

wandb_params:
  entity: null  # your wandb entity
  project: null # your wandb project
  id: null
  log_sequences_every:
    train: 10
    eval: 10
  enable_wandb: false

hydra:
  run:
    dir: ${output_dir}
